Metadata-Version: 2.4
Name: conan-dub
Version: 0.1.0
Summary: Research utility scaffold for the Detective Conan anime dubbing pipeline.
Author: Detective Conan AI Dubbing Team
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typer>=0.12
Requires-Dist: pydantic>=1.10
Requires-Dist: pyyaml>=6.0
Provides-Extra: cpu
Requires-Dist: rich>=13.0; extra == "cpu"
Requires-Dist: pydub>=0.25; extra == "cpu"
Requires-Dist: soundfile>=0.12; extra == "cpu"
Requires-Dist: pandas>=2.0; extra == "cpu"
Requires-Dist: pyarrow>=16.0; extra == "cpu"
Requires-Dist: jsonschema>=4.22; extra == "cpu"
Provides-Extra: gpu
Requires-Dist: torch>=2.0; extra == "gpu"
Requires-Dist: torchaudio>=2.0; extra == "gpu"
Requires-Dist: transformers>=4.37; extra == "gpu"
Requires-Dist: whisperx>=3.1; extra == "gpu"
Requires-Dist: pyannote.audio>=3.1; extra == "gpu"
Requires-Dist: opencv-python>=4.9; extra == "gpu"
Requires-Dist: ffmpeg-python>=0.2.0; extra == "gpu"
Requires-Dist: datasets>=2.19; extra == "gpu"
Requires-Dist: librosa>=0.10; extra == "gpu"
Provides-Extra: dev
Requires-Dist: black>=24.4; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1; extra == "dev"
Requires-Dist: mypy>=1.10; extra == "dev"
Dynamic: license-file

# Detective Conan AI Dubbing System - Production Plan

[![Windows CI](https://github.com/your-org/conan-dub/actions/workflows/windows.yml/badge.svg)](https://github.com/your-org/conan-dub/actions/workflows/windows.yml)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](notebooks/01_colab_starter.md)

**Scope:** Build an AI dubbing reference system that translates Japanese -> Italian and synthesizes multi-speaker, emotion-aware voices. Architecture based on Parakeet/Dia; preprocessing on GPU/Colab; training on TPUs (JAX/Flax NNX); synthesis & mixing locally or in cloud.

**Usage note & rights:** Only process **content you own or are licensed to use**. Outputs are for research/education and private use. Do **not** redistribute copyrighted video/audio without permission.

---

## Quickstart (Dry-Run)

1. `pip install -e .[cpu,dev]`
2. `conan-dub split input.mp4 --out work/audio --dry-run`
3. `conan-dub diarize work/audio/voice.wav --out work/diar --dry-run`
4. `conan-dub align work/audio/voice.wav --segments work/diar/segments.json --out work/aligned.json --dry-run`
5. `conan-dub fuse --aligned work/aligned.json --diar work/diar/segments.json --ser-path work/emotions.json --faces work/faces.json --out work/manifest.json --dry-run`
6. `conan-dub translate work/manifest.json --out work/manifest_it.json --dry-run`
7. `conan-dub synth work/manifest_it.json --out work/tts --dry-run`
8. `conan-dub mix work/tts --out work/final_mix.wav --dry-run`
9. `conan-dub mux input.mp4 --audio work/final_mix.wav --out output_private.mkv --dry-run`

The CLI honours `--dry-run/--no-dry-run` flags so every stage can be validated without invoking GPU-dependent models. Drop the flag when the required adapters and checkpoints are installed.

---

## Windows CPU Quickstart

```powershell
Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass
.\scripts\win\setup.ps1
.\scripts\win\demo_dry_run.ps1
.\scripts\win\test.ps1
```

Preflight warnings will highlight optional tools (for example `ffmpeg`) that become mandatory once you leave dry-run mode.

---

## Deterministic Dry-Run (No ffmpeg)

- Placeholder assets (silent WAVs, empty JSON manifests, text video stubs) are generated in pure Python, guaranteeing deterministic outputs even when `ffmpeg` or GPU runtimes are absent.
- Schema validation runs on every manifest write; violations downgrade to warnings while `--dry-run` is active.
- Verbosity can be adjusted globally with `-v/--verbose` and `-q/--quiet` to surface troubleshooting details when needed.

---

## Pipeline Overview

```
input.mp4
   |
   +--> Audio Separation (vocal | demucs | uvr5)
   |       |
   |       +--> Diarization (WhisperX + pyannote)
   |       +--> Alignment + Emotion Tags + Translation
   |
   +--> Frame Extraction --> Face Detection (AnimeFace / InsightFace)
           \--> Character Linking (seed + clustering + fusion)
   |
   +--> TTS (Parakeet/Dia) --> Mixing & Loudness --> Private mux
```

---

## Documentation Index

- [01 - Preprocessing & Dataset Creation](docs/01_preprocessing_dataset.md)
- [02 - Model Training](docs/02_model_training.md)
- [03 - Automatic Dubbing & Synthesis](docs/03_dubbing_synthesis.md)
- [04 - Engineering Notes](docs/04_engineering_notes.md)
- [05 - Codegen Prompt](docs/05_codegen_prompt.md)
- [Colab Notebook Scaffold](notebooks/01_colab_starter.md)
